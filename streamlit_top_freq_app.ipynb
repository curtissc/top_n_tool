{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ceae8707",
   "metadata": {},
   "source": [
    "## Census Testing Version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784c1bf4",
   "metadata": {},
   "source": [
    "Link to [Shakespeare data](https://www.kaggle.com/datasets/kingburrito666/shakespeare-plays) on Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fc0a4e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "# st.set_page_config(layout=\"wide\")\n",
    "st.set_page_config(page_title=\"Top Frequency Extractor\")\n",
    "import os.path\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "nltk.data.path.append('/data/cqa/nltk_data/')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "x = pd.read_csv('/home/c/chapm356/example_datasets/Shakespeare_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0087acc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(111396, 6)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "08106ff5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3600, 7)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = pd.read_csv('/home/c/chapm356/example_datasets/Shakespeare_data_short.csv')\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56b1ab31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-06 13:29:08.889 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run /apps/miniconda3/envs/nlp/lib/python3.11/site-packages/ipykernel_launcher.py [ARGUMENTS]\n",
      "2024-12-06 13:29:08.895 Session state does not function when running a script without `streamlit run`\n"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "# st.set_page_config(layout=\"wide\")\n",
    "st.set_page_config(page_title=\"Top Frequency Extractor\")\n",
    "import os.path\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "nltk.data.path.append('/data/cqa/nltk_data/')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# set stop words & punctuation\n",
    "stop_words = stopwords.words('english')\n",
    "my_punctuation = '!\"$%&\\'()*+,-./:;<=>?[\\\\]^_`’{|}~•@'\n",
    "word_rooter = nltk.stem.snowball.PorterStemmer(ignore_stopwords=False).stem\n",
    "\n",
    "df = pd.read_csv('/home/c/chapm356/example_datasets/Shakespeare_data_short.csv', index_col=0)\n",
    "df = df[['Play', 'ActSceneLine', 'Player', 'PlayerLine']]\n",
    "df_example = pd.concat([df[df['Play']==play].head(3) for play in set(df['Play'])]).reset_index(drop=True).head(10)\n",
    "\n",
    "##################################### APP CODE #########################################\n",
    "\n",
    "st.write(\"\"\"\n",
    "## Extract top *n* words from text responses\n",
    "###### *App by [Curtiss Chapman](curtiss.a.chapman@census.gov \"Email curtiss.a.chapman@census.gov with any questions\"), U.S. Census Bureau, Center for Behavioral Science Methods*\n",
    "\n",
    "With this app, you can upload a .csv file and extract the top n \n",
    "most frequent words from a response column. \n",
    "\"\"\")\n",
    "   \n",
    "with st.expander(\"Instructions\"):\n",
    "    st.write(\"\"\"\n",
    "    1. **Select file:** Drag and drop your file onto the box below (or select the file \n",
    "    via the \"Browse files\" button).\n",
    "        - A preview of your file will appear below the uploader, and a sidebar will appear.\n",
    "    2. **Select response:** In the sidebar, choose which column contains the response \n",
    "    from which you want the most frequent words.\n",
    "    3. **(Optional) Select grouping variable:** Choose which column contains groups for \n",
    "    which you want separate sets of top frequency words. If you don't have a grouping \n",
    "    variable, leave blank.\n",
    "    4. **Input desired # of most frequent words:** Set how many of the top frequency words you want from your responses.\n",
    "    5. **(Optional) Set additional words to ignore:**: If there are words you would not like to \n",
    "    include in the set of top words, type them here separated by a comma and space. Common \n",
    "    words like *the, a, an, if*, etc. are excluded by default and do not need to be written \n",
    "    in the box.  \n",
    "    6. **(Optional) Set additional words to flag:**: If there are words that you know are \n",
    "    indicative of labels/topics you care about, type them here separated by a comma and \n",
    "    space. Columns for these words will be added along with the most frequent words.  \n",
    "    7. **(Optional) Set number of most frequent 2-grams to include:** 2-grams are sets of two words separated by a space or punctuation, such as \"data science\" or \"higgledy-piggledy\". \n",
    "    8. **(Optional) Set number of most frequent 3-grams to include:** 3-grams are sets of three words separated by a space or punctuation, such as \"secretary of state\" or \"mother-in-law\". \n",
    "    9. **(Optional) Choose whether to stem words:** \"Stemming\" removes inflectional or \n",
    "    derivational prefixes and suffixes. Thus, stemming is useful if you want to treat words \n",
    "    with different forms as occurrences of the same word. For example, if you want *interrupt*, \n",
    "    *uninterrupted*, and *interruption* to all be treated as instances of the same word, you would \n",
    "    check the 'Stem' checkbox. If unchecked, these will be treated as different words for the \n",
    "    sake of frequency count.\n",
    "    10. **Press \"Get top words\":** Press the \"Get top words\" button.\n",
    "        - The words \"Processing data...\" will appear, and the top right corner of the page \n",
    "        will indicate that processing is ongoing.\n",
    "        - The results will appear in a table, which you can scroll through and inspect.\n",
    "        - A column for each top frequency word will be attached to your file to indicate \n",
    "        how many times a given word is present in the response column.\n",
    "        - If you chose a grouping factor, each group will be output into a separate table \n",
    "        that will appear in its own tab.\n",
    "    11. **Download results:** Type a name in the text input for your file, click outside the text input box, and then hit the \n",
    "    \"Download CSV\" button. Please note that if you save via the download button in the upper \n",
    "    right corner of the displayed table, only the first 50 rows of your data will be saved.\n",
    "    12. **Restart or modify results:** After processing your results, you can restart \n",
    "    the process with the \"Start Over\" button to revise your inputs. Alternatively, you can \n",
    "    click the \"X\" to the right of your uploaded file to remove that file and upload a new file.\n",
    "    \"\"\")\n",
    "    \n",
    "with st.expander(\"Data Format Example\"):\n",
    "    st.write(\"\"\"\n",
    "    An example of the expected format for input data is shown below. Your response \n",
    "    variable should be stored in a single column. Likewise your grouping variable \n",
    "    should be stored in a single column, repeated where it applies to a given response.\n",
    "    - Here, we might reasonably choose \"PlayerLine\" as the response variable. \n",
    "    - Additionally, we might choose \"Play\" as a grouping variable. \n",
    "    \n",
    "    ***NOTE:** Please make sure that your data is saved as a .csv before uploading. \n",
    "    The code will not handle other file formats.*\n",
    "    \"\"\")\n",
    "    st.table(df_example)\n",
    "\n",
    "st.write(\"### Choose file\")\n",
    "\n",
    "# upload file\n",
    "uploaded_file = st.file_uploader(\"Upload your CSV file\", type=['.csv'])\n",
    "\n",
    "# Setup or reset variables while no file is uploaded\n",
    "if uploaded_file is None:\n",
    "    # reset all session state variables\n",
    "    if 'valid_response' not in st.session_state:\n",
    "        st.session_state.valid_response = False\n",
    "    st.session_state.valid_response = False\n",
    "    \n",
    "    if 'valid_n' not in st.session_state:\n",
    "        st.session_state.valid_n = False\n",
    "    st.session_state.valid_n = False\n",
    "    \n",
    "    if 'clicked' not in st.session_state:\n",
    "        st.session_state.clicked = False\n",
    "    st.session_state.clicked = False\n",
    "    \n",
    "    if 'process_button_disabled' not in st.session_state:\n",
    "        st.session_state.process_button_disabled = False\n",
    "    st.session_state.process_button_disabled = False\n",
    "    \n",
    "    if 'restart_button_disabled' not in st.session_state:\n",
    "        st.session_state.restart_button_disabled = True\n",
    "    st.session_state.restart_button_disabled = True\n",
    "    \n",
    "    if 'processed' not in st.session_state:\n",
    "        st.session_state.processed = {}\n",
    "    st.session_state.processed = {}\n",
    "    \n",
    "# when file is uploaded, do things \n",
    "if uploaded_file is not None:\n",
    "    dataset = pd.read_csv(uploaded_file)\n",
    "    \n",
    "    # show head of uploaded data \n",
    "    st.write(\"### Uploaded Data Preview\")\n",
    "    st.dataframe(dataset.head())\n",
    "    \n",
    "    # add sidebar with drop down menus for response and grouping factor, text input for n\n",
    "    st.sidebar.write(\"## Process Your Data\")\n",
    "    \n",
    "    columns_list = dataset.columns.tolist()\n",
    "    columns_list.insert(0, '')\n",
    "    resp_col = st.sidebar.selectbox(\"Select response column\", \n",
    "                                    columns_list, \n",
    "                                    help=\"Responses should be the text from which you wish to extract the top words.\")\n",
    "    group_col = st.sidebar.selectbox(\"(Optional) Select grouping column\", \n",
    "                                     columns_list, \n",
    "                                     help=\"\"\"\n",
    "                                     Select a grouping factor if you want the top n words for each group. \n",
    "                                     Otherwise, leave blank.\n",
    "                                     \"\"\")\n",
    "    top_n = st.sidebar.text_input(\"Input desired # of most frequent words\", \n",
    "                                  value=20, \n",
    "                                  max_chars=3, \n",
    "                                  help=\"Input number must be an integer between 1 and 999.\")\n",
    "        \n",
    "    st.sidebar.write(\"#### Advanced Options (Optional)\")\n",
    "    \n",
    "    added_stopwords = st.sidebar.text_input(\"Words to ignore\", \n",
    "                                  value='', \n",
    "                                  help=\"Words typed here will be excluded from list of words added to your dataset. It is sometimes useful to review the words that have been added to your dataset in case they are not useful to you. Input lowercase words separated by comma and space to assure they are excluded from top frequency word set (e.g., census, table, like)\")\n",
    "    str_sep_added_stopwords = added_stopwords.split(', ')\n",
    "    added_keywords = st.sidebar.text_input(\"Additional words to flag\", \n",
    "                                  value='', \n",
    "                                  help=\"Words typed here will be added to the list of words added to your dataset, regardless of their frequency across responses. Input lowercase words separated by comma and space to assure they are excluded from top frequency word set (e.g., census, table, like)\")\n",
    "    str_sep_added_keywords = added_keywords.split(', ')\n",
    "    top_n2 = st.sidebar.text_input(\"Number of 2-grams (e.g. United States, hot pot)\", \n",
    "                              value=0, \n",
    "                              max_chars=3, \n",
    "                              help=\"Input number must be an integer between 1 and 999.\")\n",
    "    top_n3 = st.sidebar.text_input(\"Number of 3-grams (e.g. secretary of state, mother-in-law)\", \n",
    "                              value=0, \n",
    "                              max_chars=3, \n",
    "                              help=\"Input number must be an integer between 1 and 999.\")\n",
    "    stem_checkbox = st.sidebar.checkbox('Stem words')\n",
    "    \n",
    "    # Check that response is selected\n",
    "    if resp_col != '':\n",
    "        st.session_state.valid_response = True\n",
    "    else:\n",
    "        st.session_state.valid_response = False\n",
    "        \n",
    "    # Check for valid n\n",
    "    n_valid = not bool(re.search(r'\\D+', top_n))\n",
    "    n2_valid = not bool(re.search(r'\\D+', top_n2))\n",
    "    n3_valid = not bool(re.search(r'\\D+', top_n3))\n",
    "    if not n_valid | n2_valid | n3_valid:\n",
    "        st.sidebar.write(\"Please input valid integer for n.\")\n",
    "        st.session_state.valid_n = False\n",
    "    else:\n",
    "        top_n = int(top_n)\n",
    "        top_n2 = int(top_n2)\n",
    "        top_n3 = int(top_n3)\n",
    "        st.session_state.valid_n = True\n",
    "            \n",
    "    # Show button with valid response & n\n",
    "    def click_button():\n",
    "        st.session_state.clicked = True\n",
    "        st.session_state.process_button_disabled = True\n",
    "        \n",
    "    if (st.session_state.valid_response) & (st.session_state.valid_n):\n",
    "        st.sidebar.button(\"Get top words\", on_click=click_button, disabled=st.session_state.process_button_disabled)\n",
    "     \n",
    "    ############################## GET TOP WORDS ##############################\n",
    "    \n",
    "    # define important functions\n",
    "    def remove_stopwords(txt):\n",
    "        return ' '.join([word for word in txt.split(' ') if word not in stop_words + str_sep_added_stopwords])\n",
    "    \n",
    "    def stem_words(txt):\n",
    "        return ' '.join([word_rooter(wd) for wd in txt.split(' ')])\n",
    "    \n",
    "    def count_top_n(vec, dataset=dataset, cat='', group_col='', top_n=top_n):\n",
    "        if (cat != '') & (group_col != ''):\n",
    "            vec = vec[dataset[group_col]==cat]\n",
    "            cat = \"gp_\" + cat + '_'\n",
    "            \n",
    "        # set cap on document frequency based on dataset size\n",
    "        if len(vec) < 10000:\n",
    "            min_df = 1\n",
    "        elif len(vec) < 100000:\n",
    "            min_df = 5\n",
    "        else:\n",
    "            min_df = 10\n",
    "            \n",
    "        # set ngram range based on ngram input\n",
    "        if top_n3 > 0:\n",
    "            ngram_range = (1,3)\n",
    "        elif top_n2 > 0:\n",
    "            ngram_range = (1,2)\n",
    "        else:\n",
    "            ngram_range = (1,1)\n",
    "        count_vec = CountVectorizer(ngram_range=ngram_range, \n",
    "                              decode_error='ignore',\n",
    "                                   min_df = min_df)\n",
    "        dummy_matrix = count_vec.fit_transform(vec).toarray()\n",
    "        df_matrix = pd.DataFrame(dummy_matrix, columns=count_vec.get_feature_names_out())\n",
    "        df_matrix = df_matrix.astype(bool).astype(int) # change to doc freq before sum\n",
    "        \n",
    "        # retrieve top words based on ngram input\n",
    "        sorted_wds = df_matrix.sum(axis=0).sort_values(ascending=False)\n",
    "        sorted_1grams = sorted_wds[[wd for wd in sorted_wds.index if len(wd.split(' '))==1]]\n",
    "        top_1grams = sorted_1grams.head(top_n).index.tolist()\n",
    "        if top_n3 > 0:\n",
    "            sorted_3grams = sorted_wds[[wd for wd in sorted_wds.index if len(wd.split(' '))==3]]\n",
    "            top_3grams = sorted_3grams.head(top_n3).index.tolist()\n",
    "        else:\n",
    "            top_3grams = []\n",
    "        if top_n2 > 0:\n",
    "            sorted_2grams = sorted_wds[[wd for wd in sorted_wds.index if len(wd.split(' '))==2]]\n",
    "            top_2grams = sorted_2grams.head(top_n2).index.tolist()\n",
    "        else:\n",
    "            top_2grams = []\n",
    "        top_n_wds = top_1grams + top_2grams + top_3grams\n",
    "        \n",
    "        # add custom keywords\n",
    "        if str_sep_added_keywords[0]!='':\n",
    "            top_n_wds = top_n_wds + str_sep_added_keywords\n",
    "        df_matrix_top_n = df_matrix[top_n_wds]\n",
    "        df_matrix_top_n.columns = [f\"{cat}wd_{i+1}_{col.replace(' ', '_')}\" for i, col in enumerate(df_matrix_top_n.columns.values)]\n",
    "        response_top_n = pd.concat([vec.reset_index(drop=True), df_matrix_top_n], axis=1)\n",
    "        return response_top_n    \n",
    "    \n",
    "    # On click, get top words\n",
    "    processing_placeholder = st.empty()\n",
    "    \n",
    "    if st.session_state.clicked:\n",
    "        processing_placeholder.write(\"Processing data...\")\n",
    "        my_bar = st.progress(0, text='Subsetting data...')\n",
    "        \n",
    "        # subset dataset \n",
    "        dataset = dataset.reset_index(drop=True)\n",
    "        response = dataset[resp_col].copy() # response\n",
    "\n",
    "        # preprocess responses\n",
    "        # lowercase\n",
    "        response = response.str.lower()\n",
    "        my_bar.progress(10, text='Removing potential encoding errors...')\n",
    "        # get rid of weird encoding errors\n",
    "#         response = response.apply(lambda x: str(x).encode('cp1252', 'backslashreplace').decode('utf-8','backslashreplace'))\n",
    "        my_bar.progress(20, text='Stripping punctuation...')\n",
    "        # strip punctuation\n",
    "        response = response.apply(lambda x: re.sub(f'[{my_punctuation}]+', '', str(x)).strip())\n",
    "        my_bar.progress(30, text='Removing stopwords...')\n",
    "        # remove stopwords\n",
    "        response = response.apply(remove_stopwords)\n",
    "        my_bar.progress(40, text='Getting word counts...')\n",
    "        # stem\n",
    "        if stem_checkbox:\n",
    "            response = response.apply(stem_words)\n",
    "            my_bar.progress(60, text='Stemming words...')\n",
    "\n",
    "        # get word count across responses\n",
    "        ## if there isn't a grouping factor, treat as a single dataset\n",
    "        if len(group_col) == 0:\n",
    "            response_top_n = count_top_n(response)\n",
    "            y = pd.concat([dataset.rename(columns={resp_col:f\"{resp_col}_orig\"}), response_top_n], axis=1)\n",
    "            y = y.rename(columns={resp_col:f\"{resp_col}_clean\"})\n",
    "            my_bar.progress(100, text='Finishing up...')\n",
    "\n",
    "        ## if there is a grouping factor, get words for separate datasets\n",
    "        elif len(group_col) != 0:\n",
    "            g = dataset[group_col].copy() # grouping factor\n",
    "            dummy_matrices_d = {}\n",
    "            progress_addition = int(np.floor(50/len(set(g))))\n",
    "            for i, cat in enumerate(set(g)): \n",
    "                print(f\"Getting top {top_n} words for category: {cat} \")\n",
    "                curr_response_top_n = count_top_n(response, cat=cat, group_col=group_col)\n",
    "                curr_dataset = dataset[dataset[group_col]==cat]\n",
    "                curr_dataset = curr_dataset.drop(columns=[group_col]).rename(columns={resp_col:f\"{resp_col}_orig\"}).reset_index()\n",
    "                curr_y = pd.concat([curr_dataset, \n",
    "                                    curr_response_top_n], axis=1)\n",
    "                curr_y = curr_y.rename(columns={resp_col:f\"{resp_col}_clean\"})\n",
    "                dummy_matrices_d[cat] = curr_y\n",
    "                my_bar.progress(50+(i*progress_addition), text=f\"Getting top {top_n} words for category: {cat}...\")\n",
    "            my_bar.progress(100, text='Finishing up...')\n",
    "            y = dummy_matrices_d\n",
    "\n",
    "        # assign result to session state\n",
    "        st.session_state.processed['top_n'] = y\n",
    "        st.session_state.clicked = False\n",
    "    \n",
    "    # show that process was successful\n",
    "    if 'top_n' in st.session_state.processed:\n",
    "        st.success('Data successfully processed!', icon=\"✅\")\n",
    "        \n",
    "        processing_placeholder.empty()\n",
    "        y = st.session_state.processed['top_n']\n",
    "        \n",
    "        if isinstance(st.session_state.processed['top_n'], pd.core.frame.DataFrame):\n",
    "            st.write(f\"## Results with top {top_n} words attached\")\n",
    "            st.write(\"\"\"\n",
    "            - Scroll left and right to see table columns.\n",
    "            - Save files by clicking download icon in top right corner of the displayed table.\n",
    "            \"\"\")\n",
    "            st.dataframe(st.session_state.processed['top_n'].head(50))\n",
    "            typed_name = st.text_input(\"Input desired file name\", \n",
    "                                  value='Top_n_words_data',\n",
    "                                  help=\"File name to be saved when downloaded. Please click out of box after changing name before clicking download button. Please omit '.csv' from the name, as it will be added automatically.\")\n",
    "            st.download_button(\n",
    "                label=\"Download CSV\",\n",
    "                data=st.session_state.processed['top_n'].to_csv().encode('utf-8'),\n",
    "                file_name=f'{typed_name}.csv',\n",
    "                mime='text/csv',\n",
    "                key='1'\n",
    "            )\n",
    "        elif isinstance(y, dict):\n",
    "            st.write(f\"## Results with top {top_n} words for each group attached\")\n",
    "            st.write(\"\"\"\n",
    "            - Scroll left and right to see table columns. Hover over table to see scroll bars.\n",
    "            - Save files by clicking download icon in top right corner of the displayed table.\n",
    "            - To scroll left and right between long list of tabs, hover and \n",
    "            hold shift while using the scroll wheel. \n",
    "            \"\"\")\n",
    "            # set up tabs with results\n",
    "            n_tabs = len(y)\n",
    "            tabs = st.tabs([k for k in y])\n",
    "            for i, k in enumerate(y):\n",
    "                with tabs[i]:\n",
    "                    st.write(f\"#### {k}\")\n",
    "                    st.dataframe(y[k].head(50))\n",
    "                    typed_name = st.text_input(\"Input desired file name\", \n",
    "                                  value='Top_n_words_data',\n",
    "                                  help=\"File name to be saved when downloaded. Please click out of box after changing name before clicking download button. Please omit '.csv' from the name, as it will be added automatically.\",\n",
    "                                  key=f'name_{i}')\n",
    "                    st.download_button(\n",
    "                        label=\"Download CSV for this group\",\n",
    "                        data=y[k].to_csv().encode('utf-8'),\n",
    "                        file_name=f'{typed_name}.csv',\n",
    "                        mime='text/csv',\n",
    "                        key=f'DL_{i}'\n",
    "                    )\n",
    "        st.session_state.restart_button_disabled = False\n",
    "\n",
    "    def restart_button():\n",
    "        st.session_state.clicked = False\n",
    "        st.session_state.process_button_disabled = False\n",
    "        st.session_state.processed = {}\n",
    "        st.session_state.restart_button_disabled = True\n",
    "        \n",
    "    if not st.session_state.restart_button_disabled:\n",
    "        st.sidebar.button(\"Start Over\", on_click=restart_button, disabled=st.session_state.restart_button_disabled)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b0646f",
   "metadata": {},
   "source": [
    "## Alternative layout with 3 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5b8ea7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-19 10:26:05.616 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run /apps/miniconda3/envs/nlp/lib/python3.11/site-packages/ipykernel_launcher.py [ARGUMENTS]\n",
      "2024-04-19 10:26:05.618 Session state does not function when running a script without `streamlit run`\n"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "st.set_page_config(layout=\"wide\")\n",
    "import os.path\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "nltk.data.path.append('/data/cqa/nltk_data/')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# set stop words & punctuation\n",
    "stop_words = stopwords.words('english')\n",
    "my_punctuation = '!\"$%&\\'()*+,-./:;<=>?[\\\\]^_`’{|}~•@'\n",
    "\n",
    "##################################### APP CODE #########################################\n",
    "\n",
    "col1, col2, col3 = st.columns([4,2,4], gap='medium')\n",
    "\n",
    "col1.write(\"\"\"\n",
    "## Extract top *n* words from text responses\n",
    "###### *App by Curtiss Chapman, U.S. Census Bureau, Center for Behavioral Science Methods*\n",
    "With this app, you can upload a csv file and extract the top n \n",
    "most frequent words from a response column. The steps are simple:\n",
    "1. **Select file:** Drag and drop your file onto the box below (or select the file \n",
    "via the \"Browse files\" button).\n",
    "    - A preview of your file will appear below the uploader.\n",
    "2. **Select response:** In the sidebar, choose which column contains the response \n",
    "from which you want the most frequent words.\n",
    "3. **(Optionally) Select grouping variable:** Choose which column contains groups for \n",
    "which you want separate sets of top frequency words.\n",
    "4. **Set *n*:** Set how many of the top frequency words you want from your responses.\n",
    "5. **Press go:** Press the \"Get top words\" button.\n",
    "    - The words \"Processing data...\" will appear, and the top right corner of the page \n",
    "    will indicate that processing is ongoing.\n",
    "    - The results will appear in a table, which you can scroll through and inspect.\n",
    "    - A column for each top frequency word will be attached to your file to indicate \n",
    "    how many times a given word is present in the response column.\n",
    "    - If you chose a grouping factor, each group will be output into a separate table \n",
    "    that will appear in its own tab.\n",
    "6. **Download results:** Save results tables by hovering over the table and selecting \n",
    "the Download CSV icon that appears in the upper right corner of the table. \n",
    "(Hover over icons to see their functions).\n",
    "7. **Restart or modify results:** After processing your results, you can restart \n",
    "the process with the \"Start Over\" button to revise your inputs. Alternatively, you can \n",
    "click the \"X\" to the right of your uploaded file to remove that file and upload a new file.\n",
    "\"\"\")\n",
    "\n",
    "col2.write(\"### Choose file\")\n",
    "           \n",
    "# upload file\n",
    "uploaded_file = col2.file_uploader(\"Upload your CSV file\", type=['.csv'])\n",
    "\n",
    "# Setup or reset variables while no file is uploaded\n",
    "if uploaded_file is None:\n",
    "    # reset all session state variables\n",
    "    if 'valid_response' not in st.session_state:\n",
    "        st.session_state.valid_response = False\n",
    "    st.session_state.valid_response = False\n",
    "    \n",
    "    if 'valid_n' not in st.session_state:\n",
    "        st.session_state.valid_n = False\n",
    "    st.session_state.valid_n = False\n",
    "    \n",
    "    if 'clicked' not in st.session_state:\n",
    "        st.session_state.clicked = False\n",
    "    st.session_state.clicked = False\n",
    "    \n",
    "    if 'process_button_disabled' not in st.session_state:\n",
    "        st.session_state.process_button_disabled = False\n",
    "    st.session_state.process_button_disabled = False\n",
    "    \n",
    "    if 'restart_button_disabled' not in st.session_state:\n",
    "        st.session_state.restart_button_disabled = True\n",
    "    st.session_state.restart_button_disabled = True\n",
    "    \n",
    "    if 'processed' not in st.session_state:\n",
    "        st.session_state.processed = {}\n",
    "    st.session_state.processed = {}\n",
    "    \n",
    "# when file is uploaded, do things \n",
    "if uploaded_file is not None:\n",
    "    dataset = pd.read_csv(uploaded_file, index_col=0)\n",
    "    \n",
    "    # show head of uploaded data \n",
    "    col2.write(\"### Uploaded Data Preview\")\n",
    "    col2.dataframe(dataset.head())\n",
    "    \n",
    "    # add sidebar with drop down menus for response and grouping factor, text input for n\n",
    "    col2.write(\"## Process Your Data\")\n",
    "    \n",
    "    columns_list = dataset.columns.tolist()\n",
    "    columns_list.insert(0, '')\n",
    "    resp_col = col2.selectbox(\"Select response column\", \n",
    "                                    columns_list, \n",
    "                                    help=\"Responses should be the text from which you wish to extract the top words.\")\n",
    "    group_col = col2.selectbox(\"Select grouping column\", \n",
    "                                     columns_list, \n",
    "                                     help=\"\"\"\n",
    "                                     Select a grouping factor if you want the top n words for each group. \n",
    "                                     Otherwise, leave blank.\n",
    "                                     \"\"\")\n",
    "    top_n = col2.text_input(\"Input desired # of most frequent words\", \n",
    "                                  value=20, \n",
    "                                  max_chars=3, \n",
    "                                  help=\"Input number must be an integer between 1 and 999.\")\n",
    "    \n",
    "    # Check that response is selected\n",
    "    if resp_col != '':\n",
    "        st.session_state.valid_response = True\n",
    "    else:\n",
    "        st.session_state.valid_response = False\n",
    "        \n",
    "    # Check for valid n\n",
    "    n_valid = not bool(re.search(r'\\D+', top_n))\n",
    "    if not n_valid:\n",
    "        col2.write(\"Please input valid integer for n.\")\n",
    "        st.session_state.valid_n = False\n",
    "    else:\n",
    "        top_n = int(top_n)\n",
    "        st.session_state.valid_n = True\n",
    "            \n",
    "    # Show button with valid response & n\n",
    "    def click_button():\n",
    "        st.session_state.clicked = True\n",
    "        st.session_state.process_button_disabled = True\n",
    "        \n",
    "    if (st.session_state.valid_response) & (st.session_state.valid_n):\n",
    "        col2.button(\"Get top words\", on_click=click_button, disabled=st.session_state.process_button_disabled)\n",
    "     \n",
    "    ############################## GET TOP WORDS ##############################\n",
    "    \n",
    "    # define important functions\n",
    "    def remove_stopwords(txt):\n",
    "        return ' '.join([word for word in txt.split(' ') if word not in stop_words])\n",
    "    \n",
    "    def count_top_n(vec, dataset=dataset, cat='', group_col='', top_n=top_n):\n",
    "        if (cat != '') & (group_col != ''):\n",
    "            vec = vec[dataset[group_col]==cat]\n",
    "            cat = \"cat_\" + cat + '_'\n",
    "        count_vec = CountVectorizer(ngram_range=(1,3), \n",
    "                              decode_error='ignore')\n",
    "        dummy_matrix = count_vec.fit_transform(vec).toarray()\n",
    "        df_matrix = pd.DataFrame(dummy_matrix, columns=count_vec.get_feature_names_out())\n",
    "        top_n_wds = df_matrix.sum(axis=0).sort_values(ascending=False).head(top_n).index.values\n",
    "        df_matrix_top_n = df_matrix[top_n_wds]\n",
    "        df_matrix_top_n.columns = [f\"{cat}wd_{i+1}_{col.replace(' ', '_')}\" for i, col in enumerate(df_matrix_top_n.columns.values)]\n",
    "        response_top_n = pd.concat([vec.reset_index(drop=True), df_matrix_top_n], axis=1)\n",
    "        return response_top_n    \n",
    "    \n",
    "    # On click, get top words\n",
    "    processing_placeholder = col3.empty()\n",
    "    \n",
    "    if st.session_state.clicked:\n",
    "        processing_placeholder.write(\"Processing data...\")\n",
    "        \n",
    "        # subset dataset \n",
    "        dataset = dataset.reset_index(drop=True)\n",
    "        response = dataset[resp_col].copy() # response\n",
    "\n",
    "        # preprocess responses\n",
    "        # lowercase\n",
    "        response = response.str.lower()\n",
    "        # get rid of weird encoding errors\n",
    "        response = response.apply(lambda x: x.encode('cp1252','backslashreplace').decode('utf-8','backslashreplace'))\n",
    "        # strip punctuation\n",
    "        response = response.apply(lambda x: re.sub(f'[{my_punctuation}]+', '', x).strip())\n",
    "        # remove stopwords\n",
    "        response = response.apply(remove_stopwords)\n",
    "\n",
    "        # get word count across responses\n",
    "        ## if there isn't a grouping factor, treat as a single dataset\n",
    "        if len(group_col) == 0:\n",
    "            response_top_n = count_top_n(response)\n",
    "            y = pd.concat([dataset.drop(columns=[resp_col]), response_top_n], axis=1)\n",
    "\n",
    "        ## if there is a grouping factor, get words for separate datasets\n",
    "        elif len(group_col) != 0:\n",
    "            g = dataset[group_col].copy() # grouping factor\n",
    "            dummy_matrices_d = {}\n",
    "            for cat in set(g): \n",
    "                print(f\"Getting top {top_n} words for category: {cat} \")\n",
    "                curr_response_top_n = count_top_n(response, cat=cat, group_col=group_col)\n",
    "                curr_dataset = dataset[dataset[group_col]==cat]\n",
    "                curr_dataset = curr_dataset.drop(columns=[group_col, resp_col]).reset_index()\n",
    "                curr_y = pd.concat([curr_dataset, \n",
    "                                    curr_response_top_n], axis=1)\n",
    "                dummy_matrices_d[cat] = curr_y\n",
    "            y = dummy_matrices_d\n",
    "\n",
    "        # assign result to session state\n",
    "        st.session_state.processed['top_n'] = y\n",
    "        st.session_state.clicked = False\n",
    "    \n",
    "    # show that process was successful\n",
    "    if 'top_n' in st.session_state.processed:\n",
    "        col3.success('Data successfully processed!', icon=\"✅\")\n",
    "        \n",
    "        processing_placeholder.empty()\n",
    "        y = st.session_state.processed['top_n']\n",
    "        \n",
    "        if isinstance(st.session_state.processed['top_n'], pd.core.frame.DataFrame):\n",
    "            col3.write(f\"## Results with top {top_n} words attached\")\n",
    "            col3.write(\"\"\"\n",
    "            - Scroll left and right to see table columns.\n",
    "            - Save files by clicking download icon in top right corner of the displayed table.\n",
    "            \"\"\")\n",
    "            col3.dataframe(st.session_state.processed['top_n'])\n",
    "        elif isinstance(y, dict):\n",
    "            col3.write(f\"## Results with top {top_n} words for each group attached\")\n",
    "            col3.write(\"\"\"\n",
    "            - Scroll left and right to see table columns.\n",
    "            - Save files by clicking download icon in top right corner of the displayed table.\n",
    "            \"\"\")\n",
    "            # set up tabs with results\n",
    "            n_tabs = len(y)\n",
    "            tabs = col3.tabs([k for k in y])\n",
    "            for i, k in enumerate(y):\n",
    "                with tabs[i]:\n",
    "                    st.write(f\"#### {k}\")\n",
    "                    st.dataframe(y[k])\n",
    "        st.session_state.restart_button_disabled = False\n",
    "\n",
    "    def restart_button():\n",
    "        st.session_state.clicked = False\n",
    "        st.session_state.process_button_disabled = False\n",
    "        st.session_state.processed = {}\n",
    "        st.session_state.restart_button_disabled = True\n",
    "        \n",
    "    if not st.session_state.restart_button_disabled:\n",
    "        col2.button(\"Start Over\", on_click=restart_button, disabled=st.session_state.restart_button_disabled)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09208a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
